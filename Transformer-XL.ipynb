{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer-XL.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"11slgghPPr9sUTkrcnVLPlxkWofQpI3gm","authorship_tag":"ABX9TyPhE/qFRoYnjJ22qJ6BaXEJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["! pip install labml\n","! pip install labml-nn"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pG9qou76nn3d","executionInfo":{"status":"ok","timestamp":1641656509485,"user_tz":-420,"elapsed":6768,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}},"outputId":"f1579632-be26-4791-eb22-63122f6209c0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: labml in /usr/local/lib/python3.7/dist-packages (0.4.135)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labml) (1.19.5)\n","Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from labml) (3.1.25)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from labml) (3.13)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml) (4.0.9)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml) (3.10.0.2)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml) (5.0.0)\n","Collecting labml-nn\n","  Downloading labml_nn-0.4.118-py3-none-any.whl (316 kB)\n","\u001b[K     |████████████████████████████████| 316 kB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from labml-nn) (1.10.0+cu111)\n","Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (from labml-nn) (0.11.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from labml-nn) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from labml-nn) (1.19.5)\n","Collecting labml-helpers>=0.4.84\n","  Downloading labml_helpers-0.4.84-py3-none-any.whl (18 kB)\n","Collecting einops\n","  Downloading einops-0.3.2-py3-none-any.whl (25 kB)\n","Requirement already satisfied: labml>=0.4.135 in /usr/local/lib/python3.7/dist-packages (from labml-nn) (0.4.135)\n","Requirement already satisfied: gitpython in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.135->labml-nn) (3.1.25)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from labml>=0.4.135->labml-nn) (3.13)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.135->labml-nn) (3.10.0.2)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from gitpython->labml>=0.4.135->labml-nn) (4.0.9)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->gitpython->labml>=0.4.135->labml-nn) (5.0.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext->labml-nn) (4.62.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext->labml-nn) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml-nn) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml-nn) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml-nn) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext->labml-nn) (1.24.3)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->labml-nn) (7.1.2)\n","Installing collected packages: labml-helpers, einops, labml-nn\n","Successfully installed einops-0.3.2 labml-helpers-0.4.84 labml-nn-0.4.118\n"]}]},{"cell_type":"code","execution_count":119,"metadata":{"id":"U3G8HHyO9CeC","executionInfo":{"status":"ok","timestamp":1641665294626,"user_tz":-420,"elapsed":4330,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4d0f0ae8-c00c-478b-c1d7-e9d726d4c7bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["import math\n","from typing import Optional, List\n","import torch\n","from torch import nn as nn\n","from labml_nn.utils import clone_module_list\n","import argparse\n","from google.colab import drive\n","drive.mount('/content/gdrive/')\n","import argparse\n","import time\n","import math\n","import os, sys\n","import itertools\n","\n","import numpy as np\n","\n","import torch.optim as optim"]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/gdrive/MyDrive/Study/NLP/')\n","sys.path.append('/content/gdrive/MyDrive/Study/NLP/utils')"],"metadata":{"id":"7a9g_LKUhKKz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import utils\n","import data_utils"],"metadata":{"id":"QTIGghCVhR5r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"jlH0sX9rlcDJ"}},{"cell_type":"code","source":["class PrepareForMultiHeadAttention(nn.Module):\n","  def __init__(self, d_model, heads,d_k, bias):\n","    super().__init__()\n","    self.linear = nn.Linear(d_model, heads*d_k, bias= bias)\n","    self.heads = heads\n","    self.d_k = d_k\n","  def forward(self, x: torch.Tensor):\n","    head_shape = x.shape[:-1]\n","    x = self.linear(x)\n","    x = x.view(*head_shape, self.heads, self.d_k)\n","    return x\n","\n"],"metadata":{"id":"_IUIFyy4OFnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, heads, d_model, dropout = 0.1,bias=True):\n","      super().__init__()\n","      self.d_k = d_model // heads\n","      self.heads = heads\n","      self.query = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n","      self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n","      self.key = PrepareForMultiHeadAttention(d_model, heads, self.d_k, bias=bias)\n","      self.softmax = nn.Softmax(dim=1)\n","      self.output = nn.Linear(d_model, d_model)\n","      self.dropout = nn.Dropout(droupout)\n","      self.scale = 1/math.sqrt(self.d_k)\n","      self.attn = None\n","    def get_scores(self, query, key):\n","      return torch.einsum('ibhd,jbhd->ijbh', query, key)\n","\n","    def prepare_mask(self, mask, query_shape, key_shape):\n","      assert mask.shape[0] == 1 or mask.shape[0] == query_shape[0]\n","      assert mask.shape[1] == key_shape[0]\n","      assert mask.shape[2] == 1 or mask.shape[2] == query_shape[1]\n","\n","      mask = mask.unsqueeze(-1)\n","      return mask\n","\n","    def forward(self, *, query, key, value, mask=None):\n","      seq_len, batch_size, _ = query.shape\n","      \n","      if mask is not None:\n","        mask = self.prepare_mask(mask, query.shape, key.shape)\n","\n","      query = self.query(query)\n","      key = self.key(key)\n","      value = self.value(value)\n","      scores = self.get_scores(query, key)\n","\n","      scores *= self.scale\n","\n","      if mask is not None:\n","        scores = scores.masked_fill(mask == 0, float('-inf'))\n","\n","      attn = self.softmax(scores)  \n","      attn = self.dropout(attn)\n","      x = torch.einsum(\"ijbh,jbhd->ibhd\", attn, value)\n","      self.attn = attn.detach()\n","      x = x.reshape(seq_len, batch_size, -1)\n","      return self.output(X)"],"metadata":{"id":"I0YuGd4yXMPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class FeedForWard(nn.Module):\n","  def __init__(self, d_model, d_ff, dropout=0.1, activation=nn.ReLU(), is_gated=False, bias_1 = True, bias_2=True, bias_gate=True):\n","    super().__init__()\n","    self.layer1 = nn.Linear(d_model, d_ff, bias=bias_1)\n","    self.layer2 = nn.Linear(d_model, d_ff, bias=bias_2)\n","    self.dropout = nn.Dropout(dropout)\n","\n","    self.activation = activation\n","\n","    self.is_gated = is_gated\n","    if is_gated:\n","      self.linear_v = nn.Linear(d_model, d_ff, bias=bias_gate)\n","    \n","  def forward(self, x):\n","    g = self.activation(self.layer1(x))\n","    if self.is_gated:\n","      x = g*self.linear_v(x)\n","    else:\n","      x = g\n","    x = self.dropout(x)\n","    return self.layer2(x)"],"metadata":{"id":"c9hTbp0QTWTJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def shift_right(x):\n","  zero_pad = x.new_zeros(x.shape[0], 1, *x.shape[2:])\n","  x_padded = torch.cat([x, zero_pad], dim=1)\n","  x_padded = x_padded.view(x.shape[1] + 1, x.shape[0], *x.shape[2:])\n","  x = x_padded[:-1].view_as(x)\n","  return x\n","class RelativeMultiHeadAttention(MultiHeadAttention):\n","  def __init__(self, heads, d_model, dropout_prob = 0.1):\n","    super().__init__(heads, d_model, dropout_prob, bias=False)\n","    self.P = 2 ** 12\n","    self.key_pos_embeddings = nn.Parameter(torch.zeros((self.P * 2, heads, self.d_k)), requires_grad=True)\n","    self.key_pos_bias = nn.Parameter(torch.zeros((self.P * 2, heads)), requires_grad=True)\n","    self.query_pos_bias = nn.Parameter(torch.zeros((heads, self.d_k)), requires_grad=True)\n","\n","  def get_scores(self, query, key):\n","    key_pos_emb = self.key_pos_embeddings[self.P - key.shape[0]:self.P + query.shape[0]]\n","    key_pos_bias = self.key_pos_bias[self.P - key.shape[0]:self.P + query.shape[0]]\n","    query_pos_bias = self.query_pos_bias[None, None, :, :]\n","    \n","    ac = torch.einsum('ibhd,jbhd->ijbh', query + query_pos_bias, key)\n","    b = torch.einsum('ibhd,jhd->ijbh', query, key_pos_emb)\n","    d = key_pos_bias[None, :, None, :]\n","    bd = shift_right(b + d)\n","    bd = bd[:, -key.shape[0]:]\n","    return ac + bd\n","\n"],"metadata":{"id":"VxL_0mv6Yocp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerXLLayer(nn.Module):\n","  def __init__(self,*, d_model, self_attn, feed_forwar, dropout_prob):\n","    super().__init__()\n","    self.size = d_model\n","    self.self_attn = self_attn\n","    self.feed_forward = feed_forward\n","    self.droupout = nn.Dropout(dropout_prob)\n","    self.norm_self_attn = nn.LayerNorm([d_model])\n","    self.norm_ff = nn.LayerNorm([d_model])\n","\n","  def forward(self, *, x, mem, mask):\n","    z = self.norm_self_attn(x)\n","\n","    if mem is not None:\n","      mem = self.norm_self_attn(mem)\n","      m_z = torch.cat((mem,z), dim=0)\n","    else:\n","      m_z = z\n","    self_attn = self.self_attn(query = z, key = m_z, value = m_z, mask = mask)\n","\n","    x = x + self.dropout(self_attn)\n","    z = self.norm_ff(x)\n","    ff = self.feed_forward(z)\n","    x = x + self.dropout(ff)\n","\n","    return x\n","\n","class Transformer(nn.Module):\n","  def __init__(self, layer, n_layers):\n","    super().__init__()\n","    self.layers = clone_module_list(layer, n_layers)\n","    self.norm = nn.LayerNorm([layer.size])\n","\n","  def forward(self, x, mem, mask):\n","    new_mem = []\n","    for i, layer in enumerate(self.layers):\n","      new_mem.append(x.detach())\n","      m = mem[i] if mem else None\n","      x = layer(x=x, meme=m, mask=mask)\n","    return self.norm(x), new_mem"],"metadata":{"id":"PSlPwNJQbO_6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tiền xử lý dữ liệu"],"metadata":{"id":"G438MFdflhoc"}},{"cell_type":"code","source":["from data_utils import get_lm_corpus, LMOrderedIterator\n","from exp_utils import create_exp_dir"],"metadata":{"id":"If72q-LOTimX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["parser = argparse.ArgumentParser(description='PyTorch Transformer Language Model')\n","parser.add_argument('--data', type=str, default='/content/drive/MyDrive/Data/',\n","                    help='location of the data corpus')\n","parser.add_argument('--dataset', type=str, default='wt2',\n","                    choices=['wt103', 'lm1b', 'enwik8', 'text8'],\n","                    help='dataset name')\n","parser.add_argument('--n_layer', type=int, default=12,\n","                    help='number of total layers')\n","parser.add_argument('--n_head', type=int, default=10,\n","                    help='number of heads')\n","parser.add_argument('--d_head', type=int, default=50,\n","                    help='head dimension')\n","parser.add_argument('--d_embed', type=int, default=-1,\n","                    help='embedding dimension')\n","parser.add_argument('--d_model', type=int, default=500,\n","                    help='model dimension')\n","parser.add_argument('--d_inner', type=int, default=1000,\n","                    help='inner dimension in FF')\n","parser.add_argument('--dropout', type=float, default=0.0,\n","                    help='global dropout rate')\n","parser.add_argument('--dropatt', type=float, default=0.0,\n","                    help='attention probability dropout rate')\n","parser.add_argument('--init', default='normal', type=str,\n","                    help='parameter initializer to use.')\n","parser.add_argument('--emb_init', default='normal', type=str,\n","                    help='parameter initializer to use.')\n","parser.add_argument('--init_range', type=float, default=0.1,\n","                    help='parameters initialized by U(-init_range, init_range)')\n","parser.add_argument('--emb_init_range', type=float, default=0.01,\n","                    help='parameters initialized by U(-init_range, init_range)')\n","parser.add_argument('--init_std', type=float, default=0.02,\n","                    help='parameters initialized by N(0, init_std)')\n","parser.add_argument('--proj_init_std', type=float, default=0.01,\n","                    help='parameters initialized by N(0, init_std)')\n","parser.add_argument('--optim', default='adam', type=str,\n","                    choices=['adam', 'sgd', 'adagrad'],\n","                    help='optimizer to use.')\n","parser.add_argument('--lr', type=float, default=0.00025,\n","                    help='initial learning rate (0.00025|5 for adam|sgd)')\n","parser.add_argument('--mom', type=float, default=0.0,\n","                    help='momentum for sgd')\n","parser.add_argument('--scheduler', default='cosine', type=str,\n","                    choices=['cosine', 'inv_sqrt', 'dev_perf', 'constant'],\n","                    help='lr scheduler to use.')\n","parser.add_argument('--warmup_step', type=int, default=0,\n","                    help='upper epoch limit')\n","parser.add_argument('--decay_rate', type=float, default=0.5,\n","                    help='decay factor when ReduceLROnPlateau is used')\n","parser.add_argument('--lr_min', type=float, default=0.0,\n","                    help='minimum learning rate during annealing')\n","parser.add_argument('--clip', type=float, default=0.25,\n","                    help='gradient clipping')\n","parser.add_argument('--clip_nonemb', action='store_true',\n","                    help='only clip the gradient of non-embedding params')\n","parser.add_argument('--max_step', type=int, default=100000,\n","                    help='upper epoch limit')\n","parser.add_argument('--batch_size', type=int, default=60,\n","                    help='batch size')\n","parser.add_argument('--batch_chunk', type=int, default=1,\n","                    help='split batch into chunks to save memory')\n","parser.add_argument('--tgt_len', type=int, default=70,\n","                    help='number of tokens to predict')\n","parser.add_argument('--eval_tgt_len', type=int, default=50,\n","                    help='number of tokens to predict for evaluation')\n","parser.add_argument('--ext_len', type=int, default=0,\n","                    help='length of the extended context')\n","parser.add_argument('--mem_len', type=int, default=0,\n","                    help='length of the retained previous heads')\n","parser.add_argument('--not_tied', action='store_true',\n","                    help='do not tie the word embedding and softmax weights')\n","parser.add_argument('--seed', type=int, default=1111,\n","                    help='random seed')\n","parser.add_argument('--cuda', action='store_true',\n","                    help='use CUDA')\n","parser.add_argument('--adaptive', action='store_true',\n","                    help='use adaptive softmax')\n","parser.add_argument('--div_val', type=int, default=1,\n","                    help='divident value for adapative input and softmax')\n","parser.add_argument('--pre_lnorm', action='store_true',\n","                    help='apply LayerNorm to the input instead of the output')\n","parser.add_argument('--varlen', action='store_true',\n","                    help='use variable length')\n","parser.add_argument('--multi_gpu', action='store_true',\n","                    help='use multiple GPU')\n","parser.add_argument('--log-interval', type=int, default=200,\n","                    help='report interval')\n","parser.add_argument('--eval-interval', type=int, default=4000,\n","                    help='evaluation interval')\n","parser.add_argument('--work_dir', default='/content/drive/MyDrive/Study/NLP/', type=str,\n","                    help='experiment directory.')\n","parser.add_argument('--restart', action='store_true',\n","                    help='restart training from the saved checkpoint')\n","parser.add_argument('--restart_dir', type=str, default='',\n","                    help='restart dir')\n","parser.add_argument('--debug', action='store_true',\n","                    help='run in debug mode (do not create exp dir)')\n","parser.add_argument('--same_length', action='store_true',\n","                    help='use the same attn length for all tokens')\n","parser.add_argument('--attn_type', type=int, default=0,\n","                    help='attention type. 0 for ours, 1 for Shaw et al,'\n","                    '2 for Vaswani et al, 3 for Al Rfou et al.')\n","parser.add_argument('--clamp_len', type=int, default=-1,\n","                    help='use the same pos embeddings after clamp_len')\n","parser.add_argument('--eta_min', type=float, default=0.0,\n","                    help='min learning rate for cosine scheduler')\n","parser.add_argument('--gpu0_bsz', type=int, default=-1,\n","                    help='batch size on gpu 0')\n","parser.add_argument('--max_eval_steps', type=int, default=-1,\n","                    help='max eval steps')\n","parser.add_argument('--sample_softmax', type=int, default=-1,\n","                    help='number of samples in sampled softmax')\n","parser.add_argument('--patience', type=int, default=0,\n","                    help='patience')\n","parser.add_argument('--finetune_v2', action='store_true',\n","                    help='finetune v2')\n","parser.add_argument('--finetune_v3', action='store_true',\n","                    help='finetune v3')\n","parser.add_argument('--fp16', action='store_true',\n","                    help='Run in pseudo-fp16 mode (fp16 storage fp32 math).')\n","parser.add_argument('--static-loss-scale', type=float, default=1,\n","                    help='Static loss scale, positive power of 2 values can '\n","                    'improve fp16 convergence.')\n","parser.add_argument('--dynamic-loss-scale', action='store_true',\n","                    help='Use dynamic loss scaling.  If supplied, this argument'\n","                    ' supersedes --static-loss-scale.')\n","parser.add_argument('-f')\n","args = parser.parse_args()"],"metadata":{"id":"pwQYKtOEf9OT","executionInfo":{"status":"ok","timestamp":1641665147654,"user_tz":-420,"elapsed":376,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}}},"execution_count":115,"outputs":[]},{"cell_type":"code","source":["corpus = get_lm_corpus(args.data, 'wt2')\n","ntokens = len(corpus.vocab)\n","args.n_token = ntokens\n","\n","\n","\n","eval_batch_size = 10\n","tr_iter = corpus.get_iterator('train', args.batch_size, args.tgt_len,\n","    device=device, ext_len=args.ext_len)\n","va_iter = corpus.get_iterator('valid', eval_batch_size, args.eval_tgt_len,\n","    device=device, ext_len=args.ext_len)\n","te_iter = corpus.get_iterator('test', eval_batch_size, args.eval_tgt_len,\n","    device=device, ext_len=args.ext_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yx6Malf4gNpD","executionInfo":{"status":"ok","timestamp":1641662030793,"user_tz":-420,"elapsed":2,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}},"outputId":"c60f7e1d-022a-4773-e04f-020513f6086b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cached dataset...\n"]}]},{"cell_type":"code","source":["layer = TransformerXLLayer()\n","model = "],"metadata":{"id":"Tm36H4Zu4T_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if args.scheduler == 'cosine':\n","    # here we do not set eta_min to lr_min to be backward compatible\n","    # because in previous versions eta_min is default to 0\n","    # rather than the default value of lr_min 1e-6\n","    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n","        args.max_step, eta_min=args.eta_min) # should use eta_min arg\n","    if args.sample_softmax > 0:\n","        scheduler_sparse = optim.lr_scheduler.CosineAnnealingLR(optimizer_sparse,\n","            args.max_step, eta_min=args.eta_min) # should use eta_min arg"],"metadata":{"id":"vhcLbh-bmwa5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641662041490,"user_tz":-420,"elapsed":365,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}},"outputId":"bd99d3e4-a94d-4b70-a5cb-e4aed5eb5562"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<data_utils.LMOrderedIterator at 0x7f01bb26bed0>"]},"metadata":{},"execution_count":113}]},{"cell_type":"code","source":["if args.optim.lower() == 'adam':\n","    if args.sample_softmax > 0:\n","        dense_params, sparse_params = [], []\n","        for param in model.parameters():\n","            if param.size() == model.word_emb.weight.size():\n","                sparse_params.append(param)\n","            else:\n","                dense_params.append(param)\n","        optimizer_sparse = optim.SparseAdam(sparse_params, lr=args.lr)\n","        optimizer = optim.Adam(dense_params, lr=args.lr)\n","    else:\n","        optimizer = optim.Adam(model.parameters(), lr=args.lr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":207},"id":"GGnNkDeQ4A-L","executionInfo":{"status":"error","timestamp":1641665296843,"user_tz":-420,"elapsed":366,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}},"outputId":"70a68877-8880-47ba-9649-d30b3b9e7780"},"execution_count":120,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-120-9f7ae53fa6be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdense_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"code","source":["\n","def evaluate(eval_iter):\n","    # Turn on evaluation mode which disables dropout.\n","    model.eval()\n","\n","    # If the model does not use memory at all, make the ext_len longer.\n","    # Otherwise, make the mem_len longer and keep the ext_len the same.\n","    if args.mem_len == 0:\n","        model.reset_length(args.eval_tgt_len,\n","            args.ext_len+args.tgt_len-args.eval_tgt_len, args.mem_len)\n","    else:\n","        model.reset_length(args.eval_tgt_len,\n","            args.ext_len, args.mem_len+args.tgt_len-args.eval_tgt_len)\n","\n","    # Evaluation\n","    total_len, total_loss = 0, 0.\n","    with torch.no_grad():\n","        mems = tuple()\n","        for i, (data, target, seq_len) in enumerate(eval_iter):\n","            if args.max_eval_steps > 0 and i >= args.max_eval_steps:\n","                break\n","            ret = model(data, target, *mems)\n","            loss, mems = ret[0], ret[1:]\n","            loss = loss.mean()\n","            total_loss += seq_len * loss.float().item()\n","            total_len += seq_len\n","\n","    # Switch back to the training mode\n","    model.reset_length(args.tgt_len, args.ext_len, args.mem_len)\n","    model.train()\n","\n","    return total_loss / total_len"],"metadata":{"id":"wUtdqAfmYEZP","executionInfo":{"status":"ok","timestamp":1641665153862,"user_tz":-420,"elapsed":345,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}}},"execution_count":116,"outputs":[]},{"cell_type":"code","source":["def train():\n","    # Turn on training mode which enables dropout.\n","    global train_step, train_loss, best_val_loss, eval_start_time, log_start_time\n","    model.train()\n","    train_iter = tr_iter.get_varlen_iter() if args.varlen else tr_iter\n","    for batch, (data, target, seq_len) in enumerate(train_iter):\n","        model.zero_grad()\n","        ret = para_model(data, target, *mems)\n","        loss, mems = ret[0], ret[1:]\n","        loss = loss.float().mean().type_as(loss)\n","        loss.backward()\n","        train_loss += loss.float().item()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        optimizer.step()\n","        if args.sample_softmax > 0:\n","            optimizer_sparse.step()\n","\n","        # step-wise learning rate annealing\n","        train_step += 1\n","        if args.scheduler in ['cosine', 'constant', 'dev_perf']:\n","            # linear warmup stage\n","            if train_step < args.warmup_step:\n","                curr_lr = args.lr * train_step / args.warmup_step\n","                optimizer.param_groups[0]['lr'] = curr_lr\n","                if args.sample_softmax > 0:\n","                    optimizer_sparse.param_groups[0]['lr'] = curr_lr * 2\n","            else:\n","                if args.scheduler == 'cosine':\n","                    scheduler.step(train_step)\n","                    if args.sample_softmax > 0:\n","                        scheduler_sparse.step(train_step)\n","        elif args.scheduler == 'inv_sqrt':\n","            scheduler.step(train_step)\n","\n","        if train_step % args.log_interval == 0:\n","            cur_loss = train_loss / args.log_interval\n","            elapsed = time.time() - log_start_time\n","            log_str = '| epoch {:3d} step {:>8d} | {:>6d} batches | lr {:.3g} ' \\\n","                      '| ms/batch {:5.2f} | loss {:5.2f}'.format(\n","                epoch, train_step, batch+1, optimizer.param_groups[0]['lr'],\n","                elapsed * 1000 / args.log_interval, cur_loss)\n","            if args.dataset in ['enwik8', 'text8']:\n","                log_str += ' | bpc {:9.5f}'.format(cur_loss / math.log(2))\n","            else:\n","                log_str += ' | ppl {:9.3f}'.format(math.exp(cur_loss))\n","            logging(log_str)\n","            train_loss = 0\n","            log_start_time = time.time()\n","\n","        if train_step % args.eval_interval == 0:\n","            val_loss = evaluate(va_iter)\n","            logging('-' * 100)\n","            log_str = '| Eval {:3d} at step {:>8d} | time: {:5.2f}s ' \\\n","                      '| valid loss {:5.2f}'.format(\n","                train_step // args.eval_interval, train_step,\n","                (time.time() - eval_start_time), val_loss)\n","            if args.dataset in ['enwik8', 'text8']:\n","                log_str += ' | bpc {:9.5f}'.format(val_loss / math.log(2))\n","            else:\n","                log_str += ' | valid ppl {:9.3f}'.format(math.exp(val_loss))\n","            logging(log_str)\n","            logging('-' * 100)\n","            # Save the model if the validation loss is the best we've seen so far.\n","            if not best_val_loss or val_loss < best_val_loss:\n","                if not args.debug:\n","                    with open(os.path.join(args.work_dir, 'model.pt'), 'wb') as f:\n","                        torch.save(model, f)\n","                    with open(os.path.join(args.work_dir, 'optimizer.pt'), 'wb') as f:\n","                        torch.save(optimizer.state_dict(), f)\n","                best_val_loss = val_loss\n","\n","            # dev-performance based learning rate annealing\n","            if args.scheduler == 'dev_perf':\n","                scheduler.step(val_loss)\n","                if args.sample_softmax > 0:\n","                    scheduler_sparse.step(val_loss)\n","\n","            eval_start_time = time.time()\n","\n","        if train_step == args.max_step:\n","            break\n"],"metadata":{"id":"G8blHukRWQGO","executionInfo":{"status":"ok","timestamp":1641665156667,"user_tz":-420,"elapsed":533,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}}},"execution_count":117,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CM5ygxz_gPJK","executionInfo":{"status":"ok","timestamp":1641659043329,"user_tz":-420,"elapsed":3,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}},"outputId":"bad1f6c8-82f3-47c9-91c3-fcb8529d7b75"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[    0,   284, 15178,  ...,  1352,  1335,    16],\n","        [    1,   357,    43,  ...,    46,    43,  2015],\n","        [    2,  1496,  7369,  ...,   380,    27, 33001],\n","        ...,\n","        [  357,   415,   173,  ...,   212,    78,  1575],\n","        [ 2520,     9,  3890,  ...,   208,    27,   808],\n","        [   33,    35,    19,  ...,  8832,  6091,   209]])"]},"metadata":{},"execution_count":32}]}]}