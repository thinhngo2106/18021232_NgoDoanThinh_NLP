{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Read_data.ipynb","provenance":[],"mount_file_id":"1Fj31C_BlpzoT842xbKtvxduiInYf8qst","authorship_tag":"ABX9TyO51/bIWtyU4I2IPnnzi3tc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"eee7_DKbUBmY","executionInfo":{"status":"ok","timestamp":1641656088875,"user_tz":-420,"elapsed":1094,"user":{"displayName":"Thịnh Ngô","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GimhzgifyzsSOFlT2CwKX0z9GBBKTA4figBTZK6=s64","userId":"11625002759287276924"}}},"outputs":[],"source":["import os\n","import sys\n","import zipfile\n","\n","from io import open\n","\n","if os.path.exists('train.txt'):\n","    print('Tokenized text8 already exists - skipping processing')\n","    sys.exit()\n","\n","data = zipfile.ZipFile('/content/drive/MyDrive/Data/Data transformer/wikitext-2-v1.zip').extractall()\n"]},{"cell_type":"code","source":["data = open('text8', 'r', encoding='utf-8').read()\n","\n","print('Length of text8: {}'.format(len(data)))\n","\n","num_test_chars = 5000000\n","\n","train_data = data[: -2 * num_test_chars]\n","valid_data = data[-2 * num_test_chars: -num_test_chars]\n","test_data = data[-num_test_chars:]\n","\n","for fn, part in [('/content/drive/MyDrive/Data/train.txt', train_data), ('/content/drive/MyDrive/Data/valid.txt', valid_data), ('/content/drive/MyDrive/Data/test.txt', test_data)]:\n","    print('{} will have {} bytes'.format(fn, len(part)))\n","    print('- Tokenizing...')\n","    # Change space ' ' to underscore '_'\n","    part_str = ' '.join(['_' if c == ' ' else c for c in part.strip()])\n","    print('- Writing...')\n","    f = open(fn, 'w').write(part_str)\n","    f = open(fn + '.raw', 'w', encoding='utf-8').write(part)"],"metadata":{"id":"zALdHKC1U-ul"},"execution_count":null,"outputs":[]}]}