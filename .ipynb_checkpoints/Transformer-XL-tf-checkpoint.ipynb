{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2360,
     "status": "ok",
     "timestamp": 1642158561676,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "pbfKGd6x7KPE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import nvidia_smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1642158562210,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "Y1U_ixWfGkzE",
    "outputId": "e4e35609-d9e6-40f9-b876-51d3fb5d9557"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer XL Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1642158562211,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "c911B6eZ7PYC"
   },
   "outputs": [],
   "source": [
    "INITIALIZER = tf.keras.initializers.RandomNormal(stddev=0.01)\n",
    "\n",
    "\n",
    "def relative_mask(q_len, m_len):\n",
    "    mask = tf.sequence_mask(tf.range(1, q_len + 1), q_len, dtype=tf.float32)\n",
    "    mask = tf.pad(mask, [[0, 0], [m_len, 0]], constant_values=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def positional_embedding(k_len, d_model):\n",
    "    inv_freq = 1. / (10000 ** (tf.range(0, d_model, 2.0) / d_model))\n",
    "    pos_seq = tf.range(k_len - 1, -1, -1.0)\n",
    "    sinusoid_inp = tf.einsum('i,j->ij', pos_seq, inv_freq)\n",
    "    pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
    "    return pos_emb[None, :, :]\n",
    "\n",
    "\n",
    "def point_wise_feed_forward_network(d_model, d_ff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(d_ff, activation='relu',\n",
    "                              kernel_initializer=INITIALIZER, name='ffn1'),\n",
    "        tf.keras.layers.Dense(d_model, kernel_initializer=INITIALIZER, name='ffn2')\n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 543,
     "status": "ok",
     "timestamp": 1642158562752,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "gfaC1hRp6-w6"
   },
   "outputs": [],
   "source": [
    "class RelMultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout_rate):\n",
    "        \"\"\"\n",
    "        d_model: the number of features in the query, key and value vectors.\n",
    "        num_head: the number of heads. \n",
    "        dropout_rate: dropout\n",
    "        \"\"\"\n",
    "        super(RelMultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_depth = self.d_model // self.num_heads\n",
    "\n",
    "        self.w_head = tf.keras.layers.Dense(\n",
    "            3 * d_model, use_bias=False, kernel_initializer=INITIALIZER)\n",
    "        self.r_head = tf.keras.layers.Dense(\n",
    "            d_model, use_bias=False, kernel_initializer=INITIALIZER)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            d_model, use_bias=False, kernel_initializer=INITIALIZER)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    @staticmethod\n",
    "    def relative_shift(x):\n",
    "        x_size = tf.shape(x)\n",
    "        x = tf.pad(x, [[0, 0], [0, 0], [0, 0], [1, 0]])\n",
    "        x = tf.reshape(x, (x_size[0], x_size[1], x_size[3] + 1, x_size[2]))\n",
    "        x = tf.slice(x, [0, 0, 1, 0], [-1, -1, -1, -1])\n",
    "        x = tf.reshape(x, x_size)\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs, pos_emb, r_w_bias, r_r_bias, mems, training, **kwargs):\n",
    "        \"\"\"\n",
    "        inputs: shape=(batch_size, q_len, d_model)\n",
    "        pos_emb: shape=(1, k_len, d_model)\n",
    "        u: shape=(num_heads, d_depth)\n",
    "        v: shape=(num_heads, d_depth)\n",
    "        mems: shape=(batch_size, m_len, d_model)\n",
    "        attn_mask: shape=(m_len + q_len, q_len)\n",
    "        \"\"\"\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        q_len = tf.shape(inputs)[1]\n",
    "        # splice cache\n",
    "        if mems is None:\n",
    "            cat = inputs\n",
    "        else:\n",
    "            cat = tf.concat((mems, inputs), axis=1)\n",
    "        cat = self.dropout1(cat, training=training)\n",
    "        # k_len = m_len + q_len\n",
    "        k_len = tf.shape(cat)[1]\n",
    "        m_len = k_len - q_len\n",
    "        # shape=(1, k_len, d_model)\n",
    "        pos_emb = pos_emb[:, -k_len:]\n",
    "        pos_emb = self.dropout2(pos_emb, training=training)\n",
    "\n",
    "        w_heads = tf.reshape(self.w_head(cat), (\n",
    "            batch_size, k_len, 3 * self.num_heads, self.d_depth))\n",
    "        w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=2)\n",
    "        # shape=(batch_size, q_len, num_heads, d_depth)\n",
    "        w_head_q = w_head_q[:, -q_len:]\n",
    "\n",
    "        # shape=(batch_size, num_heads, q_len, k_len)\n",
    "        # a + c\n",
    "        ac = tf.einsum('bqnd,bknd->bnqk', w_head_q + r_w_bias, w_head_k)\n",
    "        r_head_k = tf.reshape(self.r_head(pos_emb), (k_len, self.num_heads, self.d_depth))\n",
    "        # b + d\n",
    "        bd = tf.einsum('bqnd,knd->bnqk', w_head_q + r_r_bias, r_head_k)\n",
    "        bd = self.relative_shift(bd)\n",
    "\n",
    "        attn_mask = relative_mask(q_len, m_len)\n",
    "        # shape=(batch_size, num_heads, q_len, k_len)\n",
    "        # Attention\n",
    "        attn_score = (ac + bd) / (self.d_depth ** 0.5)\n",
    "        attn_score = attn_score * attn_mask - 1e30 * (1. - attn_mask)\n",
    "        attn_score = tf.nn.softmax(attn_score, axis=-1)\n",
    "\n",
    "        attn_vec = tf.einsum('bnqk,bknd->bqnd', attn_score, w_head_v)\n",
    "        attn_vec = tf.reshape(attn_vec, (batch_size, q_len, self.d_model))\n",
    "\n",
    "        attn_out = self.dense(attn_vec)\n",
    "        return attn_out\n",
    "\n",
    "\n",
    "class TransformerLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, d_ff, num_heads, dropout_rate):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "\n",
    "        self.rel_multihead_attn = RelMultiHeadAttention(\n",
    "            d_model=d_model, num_heads=num_heads, dropout_rate=dropout_rate)\n",
    "        # feed forward network\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, d_ff)\n",
    "        # layer normalization\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        # dropout\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, pos_emb, r_w_bias, r_r_bias, mems, training, **kwargs):\n",
    "        #Attention\n",
    "        attn_out = self.rel_multihead_attn(inputs=inputs, pos_emb=pos_emb,\n",
    "                                           r_w_bias=r_w_bias, r_r_bias=r_r_bias,\n",
    "                                           mems=mems, training=training)\n",
    "        attn_out = self.dropout1(attn_out, training=training)\n",
    "        #Add the attention results\n",
    "        out1 = self.layer_norm1(inputs + attn_out)\n",
    "        # Pass through the feed-forward network\n",
    "        ffn_out = self.ffn(out1, training=training)\n",
    "        ffn_out = self.dropout2(ffn_out, training=training)\n",
    "        #Add the feed-forward results back\n",
    "        out2 = self.layer_norm2(out1 + ffn_out) \n",
    "        return out2\n",
    "\n",
    "\n",
    "class TransformerXL(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, n_vocab, d_embed, d_model, d_ff, q_len, m_len, num_heads,\n",
    "                 n_layer, dropout_rate, untie_rel_bias):\n",
    "        super(TransformerXL, self).__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.q_len = q_len\n",
    "        self.m_len = m_len\n",
    "        self.n_layer = n_layer\n",
    "        self.untie_rel_bias = untie_rel_bias\n",
    "\n",
    "        # word embedding\n",
    "        self.embedding = tf.Variable(INITIALIZER((n_vocab, d_embed)), name='embedding')\n",
    "        # word embedding size to model size\n",
    "        self.projection = tf.Variable(INITIALIZER((d_embed, d_model)), name='projection')\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "        self.pos_emb = positional_embedding(q_len + m_len, d_model)\n",
    "\n",
    "        shape = (2, n_layer if untie_rel_bias else 1, num_heads, d_model // num_heads)\n",
    "        self.rw_bias = tf.Variable(INITIALIZER(shape), name='rw_bias')\n",
    "        self.logit_bias = tf.Variable(tf.zeros((n_vocab,)), name='logit_bias')\n",
    "\n",
    "        self.multihead_layers = []\n",
    "        for i in range(n_layer):\n",
    "            layer = TransformerLayer(d_model=d_model, d_ff=d_ff, num_heads=num_heads,\n",
    "                                     dropout_rate=dropout_rate)\n",
    "            self.multihead_layers.append(layer)\n",
    "\n",
    "    def cache_mems(self, cur_out, pre_mem):\n",
    "        if self.m_len is None or self.m_len <= 0:\n",
    "            return None\n",
    "        if pre_mem is None:\n",
    "            new_mem = cur_out\n",
    "        else:\n",
    "            new_mem = tf.concat((pre_mem, cur_out), axis=1)\n",
    "        return tf.stop_gradient(new_mem[:, -self.m_len:]) # don't backpropagate to cache\n",
    "\n",
    "    def call(self, inputs, mems=None, training=False, **kwargs):\n",
    "        new_mems = []\n",
    "        x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        x = tf.matmul(x, self.projection)\n",
    "\n",
    "        if mems is None:\n",
    "            mems = [None] * self.n_layer\n",
    "\n",
    "        for i in range(self.n_layer):\n",
    "            new_mems.append(self.cache_mems(x, mems[i]))\n",
    "            j = i if self.untie_rel_bias else 0\n",
    "            x = self.multihead_layers[i](inputs=x,\n",
    "                                         pos_emb=self.pos_emb,\n",
    "                                         r_w_bias=self.rw_bias[0][j],\n",
    "                                         r_r_bias=self.rw_bias[1][j],\n",
    "                                         mems=mems[i],\n",
    "                                         training=training)\n",
    "\n",
    "        x = self.dropout1(x, training=training)\n",
    "        # share embedding parameters with inputs\n",
    "        # shape=(batch_size, seq_len, d_embed)\n",
    "        # tf.einsum('bik,jk->bij', x, self.projection)\n",
    "        x = tf.matmul(x, self.projection, transpose_b=True)\n",
    "        # shape=(batch_size, seq_len, n_vocab)\n",
    "        x = tf.matmul(x, self.embedding, transpose_b=True) + self.logit_bias\n",
    "\n",
    "        return x, new_mems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2587,
     "status": "ok",
     "timestamp": 1642158566258,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "fjdzyNJN-KVq",
    "outputId": "1db0131d-d166-4cb3-e6f5-0d8cadde41ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1642158566259,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "e4SoxpiZIicd"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Study/NLP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1642158566260,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "6YVKSrcgArnK"
   },
   "outputs": [],
   "source": [
    "from data_utils_tf import Corpus, Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 562,
     "status": "ok",
     "timestamp": 1642158566812,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "l27Q_1DRCGwQ",
    "outputId": "a4512b85-5189-46b7-bccc-c6f4e578298c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final vocab size 10000 from 28913 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "vocab_file = '/content/drive/MyDrive/Study/NLP/Data/train.txt'\n",
    "vocab_save_dir = '/content/drive/MyDrive/Study/NLP/Data/Data transformer'\n",
    "vocab = Vocab(vocab_file, vocab_save_dir, min_freq=2, max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1642158566813,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "4BWcO_gXBtQL"
   },
   "outputs": [],
   "source": [
    "VOCAB_FILE = '/content/drive/MyDrive/Study/NLP/Data/Data transformer/vocab.pkl'\n",
    "# dataset path\n",
    "DATA_PATH = '/content/drive/MyDrive/Study/NLP/Data/'\n",
    "# path for training and valid output, such as save model\n",
    "OUTPUT_PATH = '/content/drive/MyDrive/Study/NLP/output/'\n",
    "# tensorboard summary\n",
    "SUMMARY_PATH = '/content/drive/MyDrive/Study/NLP/summary/'\n",
    "BATCH_SIZE = 64\n",
    "# target length, or sequence length\n",
    "SEQ_LEN = 50\n",
    "# memory length\n",
    "MEM_LEN = 16\n",
    "# word embeeding size\n",
    "EMBEDDING_SIZE = 128\n",
    "# multihead attetion hidden size\n",
    "HIDDEN_SIZE = 128\n",
    "# feed forward network hidden size\n",
    "FFN_SIZE = 1024\n",
    "# number of heads of multiheads\n",
    "NUM_HEADS = 4\n",
    "# number of layers of multihead attention\n",
    "N_LAYER = 6\n",
    "DROPOUT_RATE = 0.1\n",
    "# wheather the bias of each layer of relative multihead attention is different or not\n",
    "UNTIE_REL_BIAS = True\n",
    "# training steps\n",
    "STEPS = 200000\n",
    "# warmup steps in the begging of training\n",
    "WARMUP_STEPS = 0\n",
    "# initial learning rate\n",
    "LEARNING_RATE = 0.0001\n",
    "# minimal learning rate\n",
    "MIN_LEARNING_RATE = 0.004\n",
    "# clips values of multiple tensors by the ratio of the sum of their norms\n",
    "CLIP_NORM = 0.25\n",
    "# number of steps between show information during training\n",
    "VERBOSE_STEP = 100\n",
    "# number of steps between save model\n",
    "SAVE_STEP = 2000\n",
    "# number of steps between verify model\n",
    "VALID_STEP = 500\n",
    "EARLY_STOPPING_TIMES = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1294,
     "status": "ok",
     "timestamp": 1642159109460,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "yoPRGUVpCkd3",
    "outputId": "0dbd07b6-f5fa-4da6-cccc-e9a48a56118a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary loaded from `/content/drive/MyDrive/Study/NLP/Data/Data transformer/vocab.pkl`\n"
     ]
    }
   ],
   "source": [
    "class CosineDecayWarmup(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "    def __init__(self, init_lr, steps, warmup_steps, min_lr):\n",
    "        super(CosineDecayWarmup, self).__init__()\n",
    "\n",
    "        self.init_lr = init_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.cosine_decay = tf.keras.experimental.CosineDecay(\n",
    "            init_lr, steps - warmup_steps, min_lr)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        linear_increase = self.init_lr * tf.cast(step, tf.float32) / (\n",
    "                tf.cast(self.warmup_steps, tf.float32) + 1e-5)\n",
    "        cosine_decay = self.cosine_decay(step)\n",
    "        return tf.cond(pred=step <= self.warmup_steps,\n",
    "                       true_fn=lambda: linear_increase,\n",
    "                       false_fn=lambda: cosine_decay)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'warmup_steps': self.warmup_steps,\n",
    "            'init_lr': self.init_lr\n",
    "        }\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    model = TransformerXL(n_vocab=corpus.vocab.size, d_embed=EMBEDDING_SIZE,\n",
    "                          d_model=HIDDEN_SIZE, d_ff=FFN_SIZE, q_len=SEQ_LEN,\n",
    "                          m_len=MEM_LEN,\n",
    "                          num_heads=NUM_HEADS, n_layer=N_LAYER, dropout_rate=DROPOUT_RATE,\n",
    "                          untie_rel_bias=UNTIE_REL_BIAS)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "corpus = Corpus(path=DATA_PATH, vocab=Vocab(VOCAB_FILE))\n",
    "model = model_fn()\n",
    "\n",
    "\n",
    "def logits_to_symbols(logits):\n",
    "    indices = np.argmax(logits, axis=-1)\n",
    "    if np.ndim(indices) <= 1:\n",
    "        indices = [indices]\n",
    "    return corpus.vocab.get_symbols(indices, join=True)\n",
    "\n",
    "\n",
    "def loss_function(labels, logits):\n",
    "    \"\"\"loss function\"\"\"\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "        labels, logits, from_logits=True)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(inputs, labels, optimizer, mems):\n",
    "    \"\"\"train a batch\"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits, new_mems = model(inputs, mems=mems, training=True)\n",
    "        loss = loss_function(labels, logits)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    clipped, gnorm = tf.clip_by_global_norm(gradients, CLIP_NORM)\n",
    "    optimizer.apply_gradients(zip(clipped, model.trainable_variables))\n",
    "\n",
    "    return loss, logits, new_mems\n",
    "\n",
    "\n",
    "def train():\n",
    "    ckpt = tf.train.Checkpoint(model=model)\n",
    "    ckpt_manager = tf.train.CheckpointManager(\n",
    "        ckpt, OUTPUT_PATH, max_to_keep=3, checkpoint_name='xl-ckpt')\n",
    "    writer = tf.summary.create_file_writer(SUMMARY_PATH)\n",
    "\n",
    "    \n",
    "    learning_rate = CosineDecayWarmup(LEARNING_RATE, STEPS, WARMUP_STEPS,\n",
    "                                      MIN_LEARNING_RATE)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "    # create corpus dataset\n",
    "    train_dataset = corpus.get_dataset('train', batch_size=BATCH_SIZE, seq_len=SEQ_LEN)\n",
    "    mems = None\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "    old_time = time.time()\n",
    "    for step, batch in enumerate(train_dataset):\n",
    "        loss, logits, mems = train_step(\n",
    "            batch['inputs'], batch['labels'], optimizer=optimizer, mems=mems)\n",
    "        train_loss(loss)\n",
    "\n",
    "\n",
    "        if step % VERBOSE_STEP == 0:\n",
    "            print('{} step: {} | loss: {:.4f} | lr: {} | {:.2f} step/s'.format(\n",
    "                time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                step,\n",
    "                train_loss.result(),\n",
    "                learning_rate(step),\n",
    "                VERBOSE_STEP / (time.time() - old_time)))\n",
    "            old_time = time.time()\n",
    "\n",
    "            with writer.as_default():\n",
    "                tf.summary.scalar('train_loss', train_loss.result(), step=step)\n",
    "            train_loss.reset_states()\n",
    "\n",
    "            inps = corpus.vocab.get_symbols(batch['inputs'], join=True)[:3]\n",
    "            outs = logits_to_symbols(logits)[:5]\n",
    "            print(inps, '\\n', outs, '\\n', sep='')\n",
    "\n",
    "        if step % SAVE_STEP == 0:\n",
    "            print('saving checkpoint for epoch {} at {}'.format(\n",
    "                step, ckpt_manager.save()))\n",
    "\n",
    "        if step % VALID_STEP == 0:\n",
    "            loss = evaluate()\n",
    "            print(f'====\\nvalidation average loss: {loss:.3f}\\n====')\n",
    "            with writer.as_default():\n",
    "                tf.summary.scalar('valid_loss', loss, step=step)\n",
    "\n",
    "        if step >= STEPS:\n",
    "            print(f'reach max step of iteations {STEPS}, training completed.')\n",
    "            break\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "\n",
    "    mems = [None] * model.n_layer\n",
    "    total_loss, total_cnt = 0., 0\n",
    "\n",
    "    valid_dataset = corpus.get_dataset('valid', batch_size=8, seq_len=SEQ_LEN)\n",
    "    for batch in valid_dataset:\n",
    "        inputs, labels = batch['inputs'], batch['labels']\n",
    "        logits, mems = model(inputs, mems=mems, training=False)\n",
    "        loss = loss_function(labels, logits)\n",
    "        # statistic total loss\n",
    "        cnt = np.prod(np.shape(labels))\n",
    "        total_cnt += cnt\n",
    "        total_loss += loss * cnt\n",
    "\n",
    "    avg_loss = total_loss / total_cnt\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "executionInfo": {
     "elapsed": 202817,
     "status": "error",
     "timestamp": 1642159317086,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "upuU5MOxCuPZ",
    "outputId": "720d4f16-d904-44ea-8201-5a39d3d0512c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-14 11:18:40 step: 0 | loss: 3.9730 | lr: 0.0 | 16.39 step/s\n",
      "['=valkyriachroniclesiii=<eos><unk>novalkyria3:<unk>chronicles(japanese:<unk>,lit.valkyriaofthebattlefield3),commonlyreferredtoasvalkyriachroniclesiiioutsidejapan,isatacticalrole@-@playingvideogamedevelopedby<unk>and<unk>', 'allthingsas<unk>ofhimself;andamun,accordingtothemythspromotedbyhis<unk>,precededandcreatedtheothercreatorgods.theseandotherversionsoftheeventsofcreationwerenotseenascontradictory.eachgivesadifferentperspectiveonthecomplex', \"inthelastsectionsofthebook.<eos>===<unk>===<eos>thestorylineconcerningtheassassinationprimarilyfollowsthefourconspiratorswhodirectlyparticipateintrujillo'sdeath.antonio<unk><unk>isoneofthefewconspiratorswhosurvivestheviolent<unk>that\"]\n",
      "['<eos>heartsiii,<eos>valkyria(<unk>chronicles(<unk><unk>(<unk>:<unk>:lit.valkyriachroniclesthebattlefield2),andknowntoasvalkyriachroniclesiii:valkyria.isamemberrole@-@tyinggamegameacebyvalkyria<unk>developed.', 'all.we.the.thethe,theto<unk><unk>ofbythe<unk>,\"by<unk>a<unk><unk>gods\\'theareother<unk>ofthegodsoftheare<unk>onlyin<unk>.<eos>ofthe<unk><unk>tothe<unk>perspective', \"<unk><unk>halfofthe<unk>was<eos>the======<eos>the<unk>ofthedeathwasresemblesthe<unk>wivesto<unk><unk>inthe's<unk>.the's,,aofthetwo<unk>who<unk>in<unk><unk>of<unk>\", 'is<unk><unk>of<unk>isthe\"theofthe<unk>.bebethe<unk>ofthe<unk>right<unk>..andtothe<unk>thatthe<unk>\"bodyelectionswas<unk>.<eos>thethe,<unk><unk><unk>the,thepartiesof<unk>', 'for,.arangeof<unk>to$<unk>.<eos>=<unk>cruiserwaswaswasthe<unk>@-@–,andafterthe<unk>,wasshedamagedher<unk>,shewaslaterdamagedandrebuilttotorangewas<unk>ayears(39ft']\n",
      "\n",
      "saving checkpoint for epoch 0 at /content/drive/MyDrive/Study/NLP/output/xl-ckpt-1\n",
      "====\n",
      "validation average loss: 4.233\n",
      "====\n",
      "2022-01-14 11:20:11 step: 100 | loss: 3.5322 | lr: 9.99999392661266e-05 | 1.10 step/s\n",
      "['allowedsafepassageinanydirectioncarryinganypersonalandpublicpropertybesides<unk>ofwar.<eos>thesoldierswouldbeallowedtomarchawayasmenleavingunderorders,notasconqueredand<unk>soldiers.<eos>onthemorningoffebruary8,1861,rector', \".themostimportanttempleimagewasthecultstatueintheinnersanctuary.thesestatueswereusuallylessthanlife@-@size,andmadeofthesamepreciousmaterialsthatweresaidtoformthegods'bodies.manytempleshadseveral<unk>,eachwith\", 'toreceivemoreattentionfrommilitary<unk>.injune1940,no.12squadronwas\"<unk>\"toformtwoadditionalunits,headquartersraafstationdarwinandno.13squadron.no.12squadronretainedits<unk>flight,whileitstwoflightsof<unk>wentto']\n",
      "['tobillsandtheway,theof<unk><unk>bills.the,the.the=<unk>werebe<unk>to<unk>tofromthecouldthethe.andonlythevillages<unk>..they=thesameofthe22,<unk>,the<unk>', \"thegodsimportant<unk>ofofthegodsofofthe<unk>temple,thewerearebuilt<unk>importantthe@-@shaped.andtheof<unk><unk><unk>stonessuchwere<unk>tobethegods'<unk>.theofandbeentemples,includingofthe\", '<unk>athan,theand.thethe1942,the2squadronwas<unk><unk>\",<unk>a<unk><unk>,includingandstation,,raaf2squadron.the79squadronwasitsheadquartersand.andeatonheadquartersbattalionswere<unk>wereto<unk>', 'jordan,thewashewas<unk>tothebullschampionshipawardforthe<unk>andjordanbullsfinishedwontothenbaofandtheywon<unk><unk><unk>bulls<unk><unk><eos>bullsalsothe<unk>,<unk><unk>his<unk><unk><unk>,,the,career', 'recordedversionof\"<unk>endgirls\"wasreleasedonnovemberunitedstatesonthe,.andthethe<unk>singleschart.numberoneonandintheweeksoftherelease.peakedsoldthechartofthechart.thewasitschartofchart']\n",
      "\n",
      "2022-01-14 11:20:16 step: 200 | loss: 3.5644 | lr: 9.999975009122863e-05 | 17.37 step/s\n",
      "[\"),forthechapelin<unk>house,ahomefor<unk>womenat<unk>,wales,andthe<unk>ofthegreat<unk>(1934)forst.george'schapel,<unk>.thefeedinghassincedisappeared,andonlyablack@-@and@-@\", '=personnel==<eos>===slayer===<eos>tom<unk>–bass,leadvocals<eos>jeff<unk>–leadandrhythmguitar<eos>kerryking–leadandrhythmguitar,backingvocals<eos>dave<unk>–drums<eos>==chartsandcertifications', ',fey<unk><unk>inasketch,alongsideamy<unk>as<unk>clinton.their<unk>includedclinton<unk><unk>abouther\"tinafeyglasses\".thesketchquicklybecamenbc\\'smost@-@watched<unk>videoever,with5@.@7millionviewsbythe']\n",
      "[\"andandthe<unk>ofthe,,and<unk>ofthe,,the,and,andthe<unk>of<unk><unk><unk>.<unk>).thelouis's,(the,<eos><unk>ofbeenbeen,andtheafew@-@<unk>@-@<unk>\", '===<eos>===music===<eos>slayerincofengineer<eos><unk>vocalsand<unk><unk>–vocalsvocalsbassguitar,<unk><unk>–drumsvocals<unk>guitar<eos><unk>vocals,<unk><unk>–<unk><eos>====certifications=', 'thewasherto<unk><unk>comedyin<unk>lee,the<unk>,feypremierewas<unk><unk>,,the<unk><unk>feyday\",feyepisodefeywasthe\\'sfirst@-@watchedprogram.game,anda@.@5millionviewersofthe<unk>', 'firstdebutwasa<unk><unk>the,<unk><unk><unk><unk>\"\",inthe<unk>,whichthetothe<unk>.theseriesof,the=====<unk>career==<eos>=wasaofthemostpopular<unk><unk>in', 'thea,\"be<unk>\"<eos>===<unk>performance===<eos>\"inredwasthe<unk>success,theunitedstates,thetowasrelease,careycritics<unk>onthatthealbumwasbebemorethetwo@,@000copies']\n",
      "\n",
      "2022-01-14 11:20:22 step: 300 | loss: 3.5343 | lr: 9.999944450100884e-05 | 17.48 step/s\n",
      "['withjust0@.@4secondsremainingtowinthegame.uponreviewofthegoalit,wasdeterminedthattheclockatstaplescenter<unk>at1@.@8secondsforoverafullsecond,whichwouldhaveresultedintime<unk>priortothegoal', 'aircraft,amateurbuiltaircraft,and<unk>.<eos>===pilotlicensing===<eos>thepilot<unk>mostrelevanttogaistheprivatepilotlicence(<unk>),which<unk>the<unk>toflyforrecreationalpurposeswithout<unk>.inadditionto', 'brick.<eos>==maincave==<eos>themaincave,alsocalledtheshivacave,cave1,orthegreatcave,is27metres(89ft)squareinplanwithahall(<unk>).attheentrancearefour']\n",
      "['a1@.@5secondsremaininginwin.game.<eos>receivingofthegame,won<unk>thethatthegamehadthecenterwasthetheam<unk>seconds.the1weekgametimeandwasbebeeninato.tothegame.', ',and<unk><unk>,and<unk>.<eos>===<unk>====<eos>theglidingpilotiscommontothewasthepilotpilotpilot.<unk>),whichisthepilot@-@theflyingpilotsflights,<unk>.theaddition,the', 'ofthe===cave==<eos>the<unk>caveiswhichknownthe<unk>cave,is<unk>,cavethe<unk>cave,isaand(<unk>ft)longindoorbya<unk>.<unk>),thetheentranceisafeet', ',were<unk>,<unk>,<unk><unk>own.well<unk>ofthe<unk>\".inthewiththeauthorities@-@communist<unk>,thewere<unk>organizationswere<unk>tothetheapril1942,the<unk><unk><unk><unk>to<unk>andthe<unk>@-@<unk>', 'no<unk><unk><unk>,byandsippykumar,whichhad<unk><unk><unk>..wasforto<unk>the<unk>.the.<unk>=thefilm,film,<unk>filmsthefilmwerethetowiththe<unk><unk><unk>yearsafterthewas,']\n",
      "\n",
      "2022-01-14 11:20:28 step: 400 | loss: 3.5721 | lr: 9.99990152195096e-05 | 17.37 step/s\n",
      "['northeastsoonafter,andendsattheintersectionofus<unk>andn@-@92.in2012,nebraskadepartmentofroads(<unk>)calculatedasmanyas2@,@<unk>vehiclestravelingonthen@-@71/n@-@88<unk>,andasfew', '<unk><unk>—featurescharacteristicof<unk>fungi.theformationofa<unk><unk>net,acharacteristicof<unk>fungi,indicatedthatg.carbonariamightbecapableofforming<unk>relationshipsundertherightconditions.<unk>andcolleaguessuggestthatitsbelow@-@groundassociationwith', \"visitors.<eos>afterdeclaringthecavesaworldheritagesite,unescogranted$100@,@000todocumentthesite'shistoryanddrawupasiteplan.apartofthegrantwasusedforconservationofthecaves.basedon<unk>byunesco,\"]\n",
      "['andaftertheandtheatthe<unk>ofm@-@,ny@-@71.thethe,thestateoftransportationwas<unk>),toaas60@,@000vehicles,(theeastern@-@71(s@-@88.<unk>whichthepartas', '..<unk>ofof<unk><unk>.<eos>fungusofthe<unk>of<unk>is<unk><unk>of<unk><unk>,isthatthecarbonariaisbe<unk>of<unk><unk>..<unk><unk>of.<eos><unk><unk><unk>thatthe<unk>@-@<unk><unk><unk>the', 'annuallythe=thethemonuments\\'\"heritagesite,theannounceda1@,@000tothethe<unk>\\'s<unk>.theattentionthenewforforthe<unk>ofthesitewasannouncedtotheofthecaves,theonthe,the\\'sthe', '<unk>,<unk>.be<unk>.the<unk><unk>oftheaustrowasthe,,atobesunkinthe.<eos>ship<unk>shipsofthetobebuiltbythe<unk><eos>20152011,theshipwasthe<unk><unk><unk><unk>inthe', '=career===<eos>=wonfirstdebut,in,,@-@<unk>,hewasinthe<unk>in<unk>,<unk>and<unk>,hehefatherwasintothe,wasthe<unk>inandinappearancesintheappearancesinhewas']\n",
      "\n",
      "2022-01-14 11:20:34 step: 500 | loss: 3.5997 | lr: 9.999846224673092e-05 | 17.36 step/s\n",
      "['wifewhogetsso<unk>happyonseeingatextmessagethatitmaywellhavecontainednewsaboutakicksequel.\"<eos>asofseptember2015,fernandezhasseveralprojectsinvariousstagesofproduction.shehascompletedshootingfor<unk><unk>\\'senglish@-@', 'othertwotypeshavedatesvaluedat$300inthatgrade.<eos>==<unk>golddollars==<eos>thegolddollarhadabrief<unk>duringtheperiodofearlyunitedstates<unk>coins.between1903and1922ninedifferentissueswereproduced,with', 'middleofrecordingtheseventh<unk>album,entitled<unk>at<unk>studiosinvancouver.<eos>==personallife==<eos>townsendhasbeenmarriedtotracyturner,his<unk>sincehewas19.shegavebirthtotheirfirstson,<unk>liam<unk>']\n",
      "[',<unk>in<unk>towiththethe<unk><unk>.heisnot<unk>ain<unk>me<unk>..\"<unk>=amarch2013,fernandezhasbeen<unk>inthestudiosofher.shealsobeenainthe,and<unk><unk>based', '<unk>werewerebeentoatthe<unk>@,@1849time.<eos>====dollar==<eos>the<unk>dollarwasa<unk><unk>,the19thofthe<unk>states,.,the<unk>and1904,dollars<unk>,recordedbywitha', 'ofthe,albumalbumalbum,a<unk>,the,,los,the===life==<eos>\"washasbornto<unk>applewhite.afirst,heisborn,hewashertothefirstalbum,<unk>,<unk>,', 'oftheoldestareafound,tothe,and<unk>in<unk>positionof<unk>.<unk><unk>.theathe<unk>,theisbeen<unk>asa<unk>inthea<unk>ofthe<unk>,andbecauseaofthe<unk>that.a', 'the<unk>thailandto<unk><unk>@-@in<unk><unk>in<unk><unk><unk>wasthatwasnotbethe<unk><unk>onbuthe<unk><unk>that<unk><unk><unk>totohelaterthathe<unk><unk>wasofthe<unk>,<eos>alsostatedthe<unk>of']\n",
      "\n",
      "====\n",
      "validation average loss: 4.244\n",
      "====\n",
      "2022-01-14 11:21:27 step: 600 | loss: 3.6048 | lr: 9.99977855826728e-05 | 1.88 step/s\n",
      "['named.the<unk>jamesp.allenestimatesthatmorethan1@,@400deitiesarenamedinegyptiantexts,whereashis<unk>christian<unk>saysthereare\"thousandsuponthousands\"ofgods.<eos>theegyptianlanguage\\'stermsforthesebeingswere<unk>,\"', \"'s<unk>,manyofwhomhadpreviouslybeengovernment<unk>,astheywaitforhiscarlatethatnight;aftertheassassination,thisstorylineshowsusthe<unk>'<unk>.eachaspectofthebook'splotrevealsadifferent<unk>onthedominican\", '2009,thatonly<unk>officers<unk>bytheirblueandyellowjacketswereinvolvedingatheringintelligenceatprotests.themetropolitanpolicetoldtheguardianthatitwasnecessarytodeployplain@-@<unk>officersto\"gatherinformationtoprovideuswitharelevantandup@-@']\n",
      "['.<eos><unk>of<unk><unk>,thatthedeitiesa@.@000deitiesare<unk>,thetradition.andthe<unk>is<unk>isthatisno<unk>of<unk>of,egyptianwho<eos>=gods<unk>ofmythsarethe<unk>is<unk>,and<unk>', '<unk>,a<unk>whomarebeenbeen<unk><unk>.werewellareforthetime.inyear.thebeingassassination,the<unk>isisthe<unk><unk>,<unk>,trujilloofofthenovelwashistoryisa<unk><unk>,urania<unk>republic', ',thattheawerewereand<unk><unk><unk><unk>jacketswere<unk>intheintelligence.the.<eos><unk>policedepartmentthe<unk>thatthewasimpossibleto<unk>a@-@<unk><unk>andthe<unk>intelligence\"theatothe<unk><unk><unk>toto', 'wasthe<unk>forthechurch,the\\'sbody.andtheevidenceofbeenrecorded.the,<unk><eos>the,1933,the<unk>was<unk>,a<unk>,thelongandwhichthewasas\"aswhichitwasunclearinthetime', '.andtreatyof<unk>,,<unk>conqueredtheinthetreatyempirechina..the<unk>.thefrance.<eos>thetreatytwoyears,<unk>,toaforceswerethe<unk><unk>thetreatyofinto<unk><unk>.<unk>.<eos>sides']\n",
      "\n",
      "2022-01-14 11:21:32 step: 700 | loss: 3.6054 | lr: 9.999698522733524e-05 | 17.25 step/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-f1030a40c6bf>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0minps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'inputs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits_to_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f1030a40c6bf>\u001b[0m in \u001b[0;36mlogits_to_symbols\u001b[0;34m(logits)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlogits_to_symbols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \"\"\"\n\u001b[0;32m-> 1188\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1642159112286,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "V1xp9j7VGevQ",
    "outputId": "9e2a32da-b6d6-4f9f-e606-709fc6813980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "restoring model from /content/drive/MyDrive/Study/NLP/output/xl-ckpt-1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f73a0102b90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = tf.train.latest_checkpoint(OUTPUT_PATH)\n",
    "print('restoring model from {}'.format(checkpoint_path))\n",
    "tf.train.Checkpoint(model=model).restore(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 82287,
     "status": "ok",
     "timestamp": 1642159401691,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "Tst8qDSB-66w"
   },
   "outputs": [],
   "source": [
    "los = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1642159401692,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "CEaA6z3iS0jG",
    "outputId": "84a242ea-1794-46a0-afef-bc7798220e41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69.53471035280468\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print(math.exp(los))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 1038,
     "status": "ok",
     "timestamp": 1642142563836,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "qmk9Y6_1GjkK"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35344,
     "status": "ok",
     "timestamp": 1642142679683,
     "user": {
      "displayName": "Ngô Thịnh",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "10951833889420027914"
     },
     "user_tz": -420
    },
    "id": "GyygsaZuUvxV",
    "outputId": "0f54c1ff-70ea-468c-e619-41ef21a8cfb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** TensorBoard Uploader *****\n",
      "\n",
      "This will upload your TensorBoard logs to https://tensorboard.dev/ from\n",
      "the following directory:\n",
      "\n",
      "/content/drive/MyDrive/Study/NLP/summary\n",
      "\n",
      "This TensorBoard will be visible to everyone. Do not upload sensitive\n",
      "data.\n",
      "\n",
      "Your use of this service is subject to Google's Terms of Service\n",
      "<https://policies.google.com/terms> and Privacy Policy\n",
      "<https://policies.google.com/privacy>, and TensorBoard.dev's Terms of Service\n",
      "<https://tensorboard.dev/policy/terms/>.\n",
      "\n",
      "This notice will not be shown again while you are logged into the uploader.\n",
      "To log out, run `tensorboard dev auth revoke`.\n",
      "\n",
      "Continue? (yes/NO) yes\n",
      "\n",
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=373649185512-8v619h5kft38l4456nm2dj4ubeqsrvh6.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email&state=LHVSccvoue1rOHstb2tfU1KKkwbRbs&prompt=consent&access_type=offline\n",
      "Enter the authorization code: 4/1AX4XfWh0COYNiyzeDGB51losdIxzdz_sXAy9oHZykU2sYm6qi7KpgFXhgkc\n",
      "\n",
      "\n",
      "New experiment created. View your TensorBoard at: https://tensorboard.dev/experiment/6CxF626rTKeCN1v6jXW4TQ/\n",
      "\n",
      "\u001b[1m[2022-01-14T06:44:29]\u001b[0m Started scanning logdir.\n",
      "\u001b[1m[2022-01-14T06:44:39]\u001b[0m Total uploaded: 1190 scalars, 0 tensors, 0 binary objects\n",
      "\u001b[1m[2022-01-14T06:44:39]\u001b[0m Done scanning logdir.\n",
      "\n",
      "\n",
      "Done. View your TensorBoard at https://tensorboard.dev/experiment/6CxF626rTKeCN1v6jXW4TQ/\n"
     ]
    }
   ],
   "source": [
    "!tensorboard dev upload \\\n",
    "  --logdir /content/drive/MyDrive/Study/NLP/summary \\\n",
    "  --name \"(optional) My latest experiment\" \\\n",
    "  --description \"(optional) Simple comparison of several hyperparameters\" \\\n",
    "  --one_shot"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformer-XL-tf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
